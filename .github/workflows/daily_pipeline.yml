name: Daily ELT Pipeline

# 1. WHEN to run:
on:
  schedule:
    - cron: '0 6 * * *'  # 6:00 AM UTC every day
  workflow_dispatch:      # Allows manual trigger

# 2. THE SETUP:
env:
  MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
  GCP_ACCESS_KEY: ${{ secrets.GCP_ACCESS_KEY }}
  GCP_SECRET_KEY: ${{ secrets.GCP_SECRET_KEY }}

jobs:
  run_dbt_job:
    runs-on: ubuntu-latest

    # Tell the robot to work inside your project subfolder
    defaults:
      run:
        working-directory: ./elt_data_pipeline_version_1

    steps:
      - name: Check out code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # C. Install dbt, DuckDB, AND the Filesystem libraries
      - name: Install dependencies
        run: |
          # fsspec, s3fs, and gcsfs are required for cloud storage connections
          pip install dbt-duckdb>=1.8.1 duckdb==1.1.2 fsspec s3fs gcsfs

      # D. Run the Pipeline
      - name: Run dbt
        run: |
          dbt deps
          dbt run --profiles-dir .